{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca76e977",
   "metadata": {},
   "source": [
    "# Neural Network and Decision Tree Analysis\n",
    "\n",
    "I will be practicing supervised learning techniques gained from the machine learning course. While insurance.csv dataset is used throughout the process, it is to be noted that I do not intend to educe any meaningful outcome from the data. The data is solely used to implement the machine learning techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685d9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "# importing packages necessary to implement neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.activations import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a69393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       age     sex     bmi  children smoker     region      charges\n",
       "0      19  female  27.900         0    yes  southwest  16884.92400\n",
       "1      18    male  33.770         1     no  southeast   1725.55230\n",
       "2      28    male  33.000         3     no  southeast   4449.46200\n",
       "3      33    male  22.705         0     no  northwest  21984.47061\n",
       "4      32    male  28.880         0     no  northwest   3866.85520\n",
       "...   ...     ...     ...       ...    ...        ...          ...\n",
       "1333   50    male  30.970         3     no  northwest  10600.54830\n",
       "1334   18  female  31.920         0     no  northeast   2205.98080\n",
       "1335   18  female  36.850         0     no  southeast   1629.83350\n",
       "1336   21  female  25.800         0     no  southwest   2007.94500\n",
       "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
       "\n",
       "[1338 rows x 7 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Datasets/insurance.csv\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972e572",
   "metadata": {},
   "source": [
    "The dataset consists of 1338 examples with 7 columns. The columns are:\n",
    "\n",
    "| Features     | Data     |\n",
    "| ----------- | ----------- |\n",
    "| age    | 18-64   |\n",
    "| sex    | female/male    |\n",
    "| bmi    | 16.0-53.1   |\n",
    "| children    | 0-5    |\n",
    "| smoker    | yes/no    |\n",
    "| region    | SE/SW/NE/NW    |\n",
    "| charges    | 1120-63800    |\n",
    "\n",
    "# Neural Network Algorithm\n",
    "\n",
    "Neural network is the algorithm that try to mimic the brain. Its composed of the input layer, hidden layer(s), and the output layer. It's to be noted that the input layer has to be composed of only numerical data. We will go through the process to turn non-numeric data to numerical data, applying one hot encoding where necessary. One-hot encoding refers to splitting up categorical data into the number of categories, making each category a binary data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dba75b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       age     bmi  children      charges  sex_male  smoker_yes  \\\n",
       "0      19  27.900         0  16884.92400         0           1   \n",
       "1      18  33.770         1   1725.55230         1           0   \n",
       "2      28  33.000         3   4449.46200         1           0   \n",
       "3      33  22.705         0  21984.47061         1           0   \n",
       "4      32  28.880         0   3866.85520         1           0   \n",
       "...   ...     ...       ...          ...       ...         ...   \n",
       "1333   50  30.970         3  10600.54830         1           0   \n",
       "1334   18  31.920         0   2205.98080         0           0   \n",
       "1335   18  36.850         0   1629.83350         0           0   \n",
       "1336   21  25.800         0   2007.94500         0           0   \n",
       "1337   61  29.070         0  29141.36030         0           1   \n",
       "\n",
       "      region_northeast  region_northwest  region_southeast  region_southwest  \n",
       "0                    0                 0                 0                 1  \n",
       "1                    0                 0                 1                 0  \n",
       "2                    0                 0                 1                 0  \n",
       "3                    0                 1                 0                 0  \n",
       "4                    0                 1                 0                 0  \n",
       "...                ...               ...               ...               ...  \n",
       "1333                 0                 1                 0                 0  \n",
       "1334                 1                 0                 0                 0  \n",
       "1335                 0                 0                 1                 0  \n",
       "1336                 0                 0                 0                 1  \n",
       "1337                 0                 1                 0                 0  \n",
       "\n",
       "[1338 rows x 10 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following code convers 'male' to 1 'female' to 0\n",
    "df_encoded = pd.get_dummies(df, columns=['sex'], dtype=int, drop_first=True)\n",
    "\n",
    "#The following code convers 'yes' to 1 'no' to 0\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['smoker'], dtype=int, drop_first=True)\n",
    "\n",
    "#The following code implements one-hot encoding on 'region' feature\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['region',], dtype=int)\n",
    "df_encoded.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcd835",
   "metadata": {},
   "source": [
    "I will implement the neural network algorithm to calculate the probability of a specific person being a smoker. Hence I need to drop the column and store in a different array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dca335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1338,)\n"
     ]
    }
   ],
   "source": [
    "smoker = df_encoded['smoker_yes'].to_numpy()\n",
    "print(smoker.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c846a9",
   "metadata": {},
   "source": [
    "Before proceeding any further, I will start by scaling every feature by z-score normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f156d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            age       bmi  children   charges  sex_male  region_northeast  \\\n",
       "0    -1.438227 -0.453151 -0.908274  0.298472 -1.010141         -0.565056   \n",
       "1    -1.509401  0.509431 -0.078738 -0.953333  0.989221         -0.565056   \n",
       "2    -0.797655  0.383164  1.580335 -0.728402  0.989221         -0.565056   \n",
       "3    -0.441782 -1.305043 -0.908274  0.719574  0.989221         -0.565056   \n",
       "4    -0.512957 -0.292447 -0.908274 -0.776512  0.989221         -0.565056   \n",
       "...        ...       ...       ...       ...       ...               ...   \n",
       "1333  0.768185  0.050278  1.580335 -0.220468  0.989221         -0.565056   \n",
       "1334 -1.509401  0.206062 -0.908274 -0.913661 -1.010141          1.768415   \n",
       "1335 -1.509401  1.014499 -0.908274 -0.961237 -1.010141         -0.565056   \n",
       "1336 -1.295877 -0.797515 -0.908274 -0.930014 -1.010141         -0.565056   \n",
       "1337  1.551106 -0.261290 -0.908274  1.310563 -1.010141         -0.565056   \n",
       "\n",
       "      region_northwest  region_southeast  region_southwest  \n",
       "0            -0.566206         -0.611095          1.764821  \n",
       "1            -0.566206          1.635183         -0.566206  \n",
       "2            -0.566206          1.635183         -0.566206  \n",
       "3             1.764821         -0.611095         -0.566206  \n",
       "4             1.764821         -0.611095         -0.566206  \n",
       "...                ...               ...               ...  \n",
       "1333          1.764821         -0.611095         -0.566206  \n",
       "1334         -0.566206         -0.611095         -0.566206  \n",
       "1335         -0.566206          1.635183         -0.566206  \n",
       "1336         -0.566206         -0.611095          1.764821  \n",
       "1337          1.764821         -0.611095         -0.566206  \n",
       "\n",
       "[1338 rows x 9 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The code below implements z-score normalization on all the features of df_encoded\n",
    "df_encoded_drop = df_encoded.drop('smoker_yes', axis=1)\n",
    "df_z_scaled = df_encoded_drop.copy()\n",
    "\n",
    "for column in df_z_scaled.columns:\n",
    "    df_z_scaled[column] = (df_z_scaled[column]-df_z_scaled[column].mean()) / df_z_scaled[column].std()\n",
    "    \n",
    "df_z_scaled.head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec9e320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating nparray of all necessary features\n",
    "x_train = df_z_scaled[['age', 'bmi', 'children', 'charges', 'sex_male', 'region_northeast', 'region_northwest', \n",
    "                      'region_southeast', 'region_southwest']].to_numpy()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7edfb3-ce37-4450-bfb9-2e131c017c93",
   "metadata": {},
   "source": [
    "Each neuron has an activation. The activation can be linear, sigmoid or relu. Since the neural network aims to predict the chances of a person being a smoker, we will use sigmoid activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6c01d3-621e-4964-8479-1c9a44735766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(9,)),\n",
    "        #Create a dense layer with sigmoid activation, which is good for binary situations.\n",
    "        Dense(3, activation='sigmoid', name='layer1'),\n",
    "        Dense(1, activation='sigmoid', name='layer2')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c565e18-7f3e-45cb-922d-4ea7f3513f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 3)                 30        \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34\n",
      "Trainable params: 34\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d57e53c-a607-4a38-971a-8d3381ece85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1(9, 3):\n",
      " [[-0.0243066  -0.6859954   0.5371651 ]\n",
      " [-0.46246472 -0.33147198  0.54098004]\n",
      " [ 0.322226   -0.69254774 -0.66457164]\n",
      " [-0.06176066  0.36415774  0.04277718]\n",
      " [ 0.35824233 -0.3874821  -0.5155927 ]\n",
      " [-0.6085216   0.6997232   0.3565182 ]\n",
      " [-0.00584257  0.42300636  0.40524262]\n",
      " [-0.16720092  0.31298     0.66041654]\n",
      " [-0.5707931  -0.4807679  -0.70085335]] \n",
      "b1(3,): [0. 0. 0.]\n",
      "W2(3, 1):\n",
      " [[ 0.2804159]\n",
      " [-0.6430382]\n",
      " [ 0.7749101]] \n",
      "b2(1,): [0.]\n"
     ]
    }
   ],
   "source": [
    "#Describes the random biases and weights Tensorflow has initiated\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "505c9fdf-ffea-4a90-9248-7864b5d19db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/42 [==============================] - 0s 292us/step - loss: 0.1252\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 281us/step - loss: 0.1177\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 235us/step - loss: 0.1111\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 214us/step - loss: 0.1076\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 226us/step - loss: 0.1062\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 230us/step - loss: 0.1050\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 226us/step - loss: 0.1030\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 242us/step - loss: 0.1022\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 215us/step - loss: 0.1010\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 222us/step - loss: 0.1010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29b096850>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01),\n",
    ")\n",
    "#epochs means the entire data set should be applied during training 10 times.\n",
    "model.fit(\n",
    "    x_train,smoker,            \n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98ced239-d595-4890-92c3-4530d1b5402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1(9, 3):\n",
      " [[ 0.97016275  0.85319906 -0.9603485 ]\n",
      " [ 1.4447821   1.4506173  -1.4787594 ]\n",
      " [ 0.16927463  0.22694588 -0.1928326 ]\n",
      " [-3.2898934  -2.8743703   3.203796  ]\n",
      " [-0.0976601  -0.22181818  0.13050695]\n",
      " [-0.2908088   0.31043464  0.10585709]\n",
      " [-0.35587466  0.26202214  0.16500361]\n",
      " [-0.40883565  0.13569534  0.23385766]\n",
      " [-0.40078142  0.2355054   0.21645196]] \n",
      "b1(3,): [ 1.4255282  1.3723803 -1.46342  ]\n",
      "W2(3, 1):\n",
      " [[-3.2533317]\n",
      " [-4.443698 ]\n",
      " [ 2.9462836]] \n",
      "b2(1,): [-0.23018911]\n"
     ]
    }
   ],
   "source": [
    "#After fitting, the weights have been updated\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62eaff9-8a76-43a8-881c-973dce68ab65",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Since now we have a trained model, we can use it to make predictions. Since this model predicts a probability, in order to make decision there has to be a threshold. We will set 0.5 as the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4cb78833-8602-4ca4-93ee-50fa8815f754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9)\n",
      "(1, 9)\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "predictions = \n",
      " [[4.8179663e-04]\n",
      " [7.3478907e-01]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([\n",
    "    [18, 33.770, 1, 1725.55230, 1, 0, 1, 0, 0],  # neg example\n",
    "    [19, 27.900, 0, 16884.92400, 0, 0, 0, 1, 0]])   # pos example\n",
    "print(X_test.shape)\n",
    "col_means = np.mean(df_encoded_drop, axis=0)\n",
    "col_means = col_means.values.reshape(1,-1)\n",
    "print(col_means.shape)\n",
    "col_std = np.std(df_encoded_drop, axis=0)\n",
    "col_std = col_std.values.reshape(1,-1)\n",
    "X_testn =(X_test - col_means) / col_std\n",
    "\n",
    "\n",
    "predictions = model.predict(X_testn)\n",
    "print(\"predictions = \\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7a9ece-9a96-4f85-9e6e-dfa36225294c",
   "metadata": {},
   "source": [
    "To convert the probabilities to a decision, we apply a threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe94addb-9fd7-4687-9cde-71f9983fa545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decisions = \n",
      "[[0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "yhat = (predictions >= 0.5).astype(int)\n",
    "print(f\"decisions = \\n{yhat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9cfd66-7276-41a9-9220-2d564e822e74",
   "metadata": {},
   "source": [
    "It can be seen that the neural network was able to predict the probabilities of a person being a smoker. Now what if what I'm trying to guess is not binary, but categorical? This can be achieved by setting the activation as softmax function. For the given data, I will implement softmax function to predict the number of kids a person has based on features such as insurance cost, age, etc. \n",
    "\n",
    "# Multiclass Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0666ebae-fba6-421f-9e40-304b757e8d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       age     sex     bmi  children smoker     region      charges\n",
       "0      19  female  27.900         0    yes  southwest  16884.92400\n",
       "1      18    male  33.770         1     no  southeast   1725.55230\n",
       "2      28    male  33.000         3     no  southeast   4449.46200\n",
       "3      33    male  22.705         0     no  northwest  21984.47061\n",
       "4      32    male  28.880         0     no  northwest   3866.85520\n",
       "...   ...     ...     ...       ...    ...        ...          ...\n",
       "1333   50    male  30.970         3     no  northwest  10600.54830\n",
       "1334   18  female  31.920         0     no  northeast   2205.98080\n",
       "1335   18  female  36.850         0     no  southeast   1629.83350\n",
       "1336   21  female  25.800         0     no  southwest   2007.94500\n",
       "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
       "\n",
       "[1338 rows x 7 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading csv file and storing to a new variable\n",
    "df2 = pd.read_csv(\"Datasets/insurance.csv\")\n",
    "df2.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f0c471-6508-4e50-951d-baf66e889202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       age     bmi  children      charges  sex_male  smoker_yes  \\\n",
       "0      19  27.900         0  16884.92400         0           1   \n",
       "1      18  33.770         1   1725.55230         1           0   \n",
       "2      28  33.000         3   4449.46200         1           0   \n",
       "3      33  22.705         0  21984.47061         1           0   \n",
       "4      32  28.880         0   3866.85520         1           0   \n",
       "...   ...     ...       ...          ...       ...         ...   \n",
       "1333   50  30.970         3  10600.54830         1           0   \n",
       "1334   18  31.920         0   2205.98080         0           0   \n",
       "1335   18  36.850         0   1629.83350         0           0   \n",
       "1336   21  25.800         0   2007.94500         0           0   \n",
       "1337   61  29.070         0  29141.36030         0           1   \n",
       "\n",
       "      region_northeast  region_northwest  region_southeast  region_southwest  \n",
       "0                    0                 0                 0                 1  \n",
       "1                    0                 0                 1                 0  \n",
       "2                    0                 0                 1                 0  \n",
       "3                    0                 1                 0                 0  \n",
       "4                    0                 1                 0                 0  \n",
       "...                ...               ...               ...               ...  \n",
       "1333                 0                 1                 0                 0  \n",
       "1334                 1                 0                 0                 0  \n",
       "1335                 0                 0                 1                 0  \n",
       "1336                 0                 0                 0                 1  \n",
       "1337                 0                 1                 0                 0  \n",
       "\n",
       "[1338 rows x 10 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following code convers 'male' to 1 'female' to 0\n",
    "df2_encoded = pd.get_dummies(df2, columns=['sex'], dtype=int, drop_first=True)\n",
    "\n",
    "#The following code convers 'yes' to 1 'no' to 0\n",
    "df2_encoded = pd.get_dummies(df2_encoded, columns=['smoker'], dtype=int, drop_first=True)\n",
    "\n",
    "#The following code implements one-hot encoding on 'region' feature\n",
    "df2_encoded = pd.get_dummies(df2_encoded, columns=['region',], dtype=int)\n",
    "df2_encoded.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5993d8cf-659c-434c-9e1c-82221ac7d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing 'children' column to y_train\n",
    "y_train = df2_encoded['children'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acad07d0-86e7-40b2-8b09-488bca8b9b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            age       bmi   charges  sex_male  smoker_yes  region_northeast  \\\n",
       "0    -1.438227 -0.453151  0.298472 -1.010141    1.969850         -0.565056   \n",
       "1    -1.509401  0.509431 -0.953333  0.989221   -0.507273         -0.565056   \n",
       "2    -0.797655  0.383164 -0.728402  0.989221   -0.507273         -0.565056   \n",
       "3    -0.441782 -1.305043  0.719574  0.989221   -0.507273         -0.565056   \n",
       "4    -0.512957 -0.292447 -0.776512  0.989221   -0.507273         -0.565056   \n",
       "...        ...       ...       ...       ...         ...               ...   \n",
       "1333  0.768185  0.050278 -0.220468  0.989221   -0.507273         -0.565056   \n",
       "1334 -1.509401  0.206062 -0.913661 -1.010141   -0.507273          1.768415   \n",
       "1335 -1.509401  1.014499 -0.961237 -1.010141   -0.507273         -0.565056   \n",
       "1336 -1.295877 -0.797515 -0.930014 -1.010141   -0.507273         -0.565056   \n",
       "1337  1.551106 -0.261290  1.310563 -1.010141    1.969850         -0.565056   \n",
       "\n",
       "      region_northwest  region_southeast  region_southwest  \n",
       "0            -0.566206         -0.611095          1.764821  \n",
       "1            -0.566206          1.635183         -0.566206  \n",
       "2            -0.566206          1.635183         -0.566206  \n",
       "3             1.764821         -0.611095         -0.566206  \n",
       "4             1.764821         -0.611095         -0.566206  \n",
       "...                ...               ...               ...  \n",
       "1333          1.764821         -0.611095         -0.566206  \n",
       "1334         -0.566206         -0.611095         -0.566206  \n",
       "1335         -0.566206          1.635183         -0.566206  \n",
       "1336         -0.566206         -0.611095          1.764821  \n",
       "1337          1.764821         -0.611095         -0.566206  \n",
       "\n",
       "[1338 rows x 9 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping 'children' column, then implementing z-score normalization on df2_encoded\n",
    "df2_encoded_drop = df2_encoded.drop('children', axis=1)\n",
    "df2_z_scaled = df2_encoded_drop.copy()\n",
    "\n",
    "for column in df2_z_scaled.columns:\n",
    "    df2_z_scaled[column] = (df2_z_scaled[column]-df2_z_scaled[column].mean()) / df2_z_scaled[column].std()\n",
    "    \n",
    "df2_z_scaled.head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc507dd3-0617-40b5-83ce-650474de1dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating and storing all features to nparray\n",
    "x_train = df2_z_scaled[['age', 'bmi', 'charges', 'sex_male', 'smoker_yes', \n",
    "                        'region_northeast', 'region_northwest', 'region_southeast', 'region_southwest']].to_numpy()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad95f92e-ee7b-4ea1-8267-4766e5d5334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/42 [==============================] - 0s 362us/step - loss: 1.8747\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 316us/step - loss: 1.6225\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 296us/step - loss: 1.5077\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 316us/step - loss: 1.4428\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 322us/step - loss: 1.4066\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 323us/step - loss: 1.3891\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 317us/step - loss: 1.3779\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 312us/step - loss: 1.3702\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 324us/step - loss: 1.3625\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 329us/step - loss: 1.3546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a225bc10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(6, activation = 'linear')    # < softmax activation here\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea9b0dc7-2a52-4c5f-a207-57eb0583d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 297us/step\n",
      "two example output vectors:\n",
      " [[ 2.5756073   1.140963    0.821749    0.5840602  -1.5949247  -2.1311479 ]\n",
      " [ 2.05817     1.365968    0.56684935  0.48262838 -1.6258609  -1.4897108 ]]\n",
      "largest value 2.8685749 smallest value -3.3586311\n"
     ]
    }
   ],
   "source": [
    "p_preferred = model.predict(x_train)\n",
    "print(f\"two example output vectors:\\n {p_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43950544-13d3-432f-9eb8-e3f3e054f2aa",
   "metadata": {},
   "source": [
    "The example output vectors are not probabilities. The output must be sent through a softmax function when performing prediction that expects a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92ebaa30-3546-4153-991f-ed784bd96199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two example output vectors:\n",
      " [[0.6360243  0.15150103 0.11009884 0.08680721 0.00982278 0.00574587]\n",
      " [0.50343585 0.25195596 0.11331093 0.10415858 0.01264707 0.01449169]]\n",
      "largest value 0.63833654 smallest value 0.0012424922\n"
     ]
    }
   ],
   "source": [
    "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
    "print(f\"two example output vectors:\\n {sm_preferred[:2]}\")\n",
    "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee90ae35-a812-4fb4-a24b-9261403c4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.5756073  1.140963   0.821749   0.5840602 -1.5949247 -2.1311479], category: 0\n",
      "[ 2.05817     1.365968    0.56684935  0.48262838 -1.6258609  -1.4897108 ], category: 0\n",
      "[ 1.8462933   1.4596443   0.82461405  0.36059633 -1.6800543  -1.2358103 ], category: 0\n",
      "[ 1.4287852   1.1868529   1.3219203   0.44475603 -1.166343   -1.5011184 ], category: 0\n",
      "[ 1.5756184   1.364863    0.96450603  0.3092387  -0.9414451  -1.342018  ], category: 0\n"
     ]
    }
   ],
   "source": [
    "#to select the most likely category, softmax is not required. \n",
    "for i in range(5):\n",
    "    print( f\"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553e646-e509-4e65-b461-b4235f25c909",
   "metadata": {},
   "source": [
    "It can be seen that the neural network predicts the first 5 examples of the x_train to be all in category 0. In other words, the network predicts, given features stored in x_train, first 5 examples to have 0 child. But we know that is not certainly true. 3rd person has 3 children, 2nd person 1 child. This may suggest that the neural network is not accurate, perhaps we can logically deduce that the features stored in x_train such as age, bmi, insurance cost, and smoker were not very good predictors for assuming the number of children."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b5dc4-9872-49f3-9dd8-42d34fc233bd",
   "metadata": {},
   "source": [
    "# Model evaluation and selection\n",
    "\n",
    "How do we know, if the architecture for the 'model' neural network is optimal for calculating the probabilities of a person being a smoker, or predicting the number of children a person has as seen in previous examples? In order to answer such critical questions, we have to be able to evaluate a model or an architecture. Here I aim to implement code to achieve such result. I will use 'x_train' and 'smoker' data used in predicting the probability of a person being a smoker to conduct the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285c5a58-85b3-4be2-ae3d-6c57322c2ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the inputs x is: (1338, 9)\n",
      "the shape of the targets y is: (1338,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"the shape of the inputs x is: {x_train.shape}\")\n",
    "print(f\"the shape of the targets y is: {smoker.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1217a97-b56f-4294-999a-83aec6e6f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (802, 9)\n",
      "the shape of the training set (target) is: (802,)\n",
      "\n",
      "the shape of the cross validation set (input) is: (268, 9)\n",
      "the shape of the cross validation set (target) is: (268,)\n",
      "\n",
      "the shape of the test set (input) is: (268, 9)\n",
      "the shape of the test set (target) is: (268,)\n"
     ]
    }
   ],
   "source": [
    "#This process splits the data into the training, cross validation, and test sets.\n",
    "\n",
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
    "x_bc_train, x_, y_bc_train, y_ = train_test_split(x_train, smoker, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_bc_cv, x_bc_test, y_bc_cv, y_bc_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_bc_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_bc_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_bc_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_bc_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_bc_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_bc_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a708f-357e-44f2-afea-e7a8b9888be1",
   "metadata": {},
   "source": [
    "The evaluation of the error for classification model will be measured by getting the fraction of the data the model has misclassified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292e709f-388f-449a-b258-3d71dba55b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    \n",
    "    model_1 = Sequential(\n",
    "        [\n",
    "            Dense(25, activation = 'relu'),\n",
    "            Dense(15, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_1'\n",
    "    )\n",
    "\n",
    "    model_2 = Sequential(\n",
    "        [\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_2'\n",
    "    )\n",
    "\n",
    "    model_3 = Sequential(\n",
    "        [\n",
    "            Dense(32, activation = 'relu'),\n",
    "            Dense(16, activation = 'relu'),\n",
    "            Dense(8, activation = 'relu'),\n",
    "            Dense(4, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_3'\n",
    "    )\n",
    "    \n",
    "    model_list = [model_1, model_2, model_3]\n",
    "    \n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd487435-beea-40d3-9b91-8cc617e6eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model_1...\n",
      "Done!\n",
      "\n",
      "26/26 [==============================] - 0s 208us/step\n",
      "9/9 [==============================] - 0s 257us/step\n",
      "Training model_2...\n",
      "Done!\n",
      "\n",
      "26/26 [==============================] - 0s 214us/step\n",
      "9/9 [==============================] - 0s 267us/step\n",
      "Training model_3...\n",
      "Done!\n",
      "\n",
      "26/26 [==============================] - 0s 222us/step\n",
      "9/9 [==============================] - 0s 269us/step\n",
      "Model 1: Training Set Classification Error: 0.32828, CV Set Classification Error: 0.31961\n",
      "Model 2: Training Set Classification Error: 0.33486, CV Set Classification Error: 0.33560\n",
      "Model 3: Training Set Classification Error: 0.33705, CV Set Classification Error: 0.33103\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_error = []\n",
    "nn_cv_error = []\n",
    "\n",
    "# Build the models\n",
    "models_bc = build_models()\n",
    "\n",
    "# Loop over each model\n",
    "for model in models_bc:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.01),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_bc_train, y_bc_train,\n",
    "        epochs=200,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    # Set the threshold for classification\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Record the fraction of misclassified examples for the training set\n",
    "    yhat = model.predict(x_bc_train)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    train_error = np.mean(yhat != y_bc_train)\n",
    "    nn_train_error.append(train_error)\n",
    "\n",
    "    # Record the fraction of misclassified examples for the cross validation set\n",
    "    yhat = model.predict(x_bc_cv)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    cv_error = np.mean(yhat != y_bc_cv)\n",
    "    nn_cv_error.append(cv_error)\n",
    "\n",
    "# Print the result\n",
    "for model_num in range(len(nn_train_error)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
    "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fa13a39-a88d-4e22-94bd-646c56ab0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 610us/step\n",
      "Selected Model: 1\n",
      "Training Set Classification Error: 0.3283\n",
      "CV Set Classification Error: 0.3196\n",
      "Test Set Classification Error: 0.3349\n"
     ]
    }
   ],
   "source": [
    "# Select the model with the lowest error\n",
    "model_num = 1\n",
    "\n",
    "# Compute the test error\n",
    "yhat = models_bc[model_num-1].predict(x_bc_test)\n",
    "yhat = tf.math.sigmoid(yhat)\n",
    "yhat = np.where(yhat >= threshold, 1, 0)\n",
    "nn_test_error = np.mean(yhat != y_bc_test)\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
    "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
    "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec99ea5-09cc-48d8-9dae-3739ed77f0e2",
   "metadata": {},
   "source": [
    "This allows a determine which of the 3 models are able to predict the outcome with the smallest error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e30954-aaa6-4da4-89b1-93921c674885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
